{"version":3,"file":"static/webpack/static\\development\\pages\\project\\scraping.js.87ffa4214fea8739db46.hot-update.js","sources":["webpack:///./pages/project/scraping.js"],"sourcesContent":["import PageLayout from '../../HOC/pageLayout'\r\n\r\n\r\nconst pageDetails = {\r\n    title: 'Sedi Web Scraping',\r\n    back:'/#projects',\r\n    next:'/project/tracker',\r\n    nextTitle:'Tracker App'\r\n  }\r\n\r\nconst LouisRiel = () => {\r\n    return (\r\n        <PageLayout pageDetails={pageDetails}>\r\n            <p>\r\n                For my application to Mawer Investment Management I was given a programming challenge as part of\r\n                my interview process. The challenge was to get all the Insider information from the SEDI website\r\n                (System for Electronic Disclosure by Insiders). \r\n            </p>\r\n            <p>\r\n                After doing some research (SEDI doesn't have any external APIs), I chose Python and the \r\n                'Beautiful Soup' library to conduct the web scraping. \r\n            </p> \r\n            <p>\r\n                Retrieving the information I needed was a little tricky because SEDI doesn't contain pagination and \r\n                there is no set url for beginning the HTML parsing. I ended up taking a SEDI network request and modifying \r\n                it using Postman to understand how the SEDI website grabbed information. If too many insiders were returned,\r\n                the request would fail causing another problem. After understanding how the queries worked\r\n                I was able to set up a script that would iterate through all the records while throttling the requests to\r\n                prevent failure.    \r\n            </p>    \r\n            <p>\r\n                While taking a look at the site through Chrome Devtools I noticed that many of the table rows and cells that \r\n                needed to be scrapped didn't contain any identifying attributes besides the width. This caused a headache, but\r\n                it was still manageable. after the records had been scrapped from the page, they were inserted into a MySQL \r\n                database for further querying. \r\n            </p>\r\n            <p>\r\n                <b>Here is a link to the github page</b>\r\n            </p>\r\n        </PageLayout>\r\n    )\r\n}\r\n\r\nexport default LouisRiel"],"mappings":";;;;;;;;;;;;;;;;;AAAA;AAGA;AACA;AACA;AACA;AACA;AAJA;AACA;AAMA;AACA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAIA;AAAA;AAAA;AAAA;AAAA;AAAA;AAQA;AAAA;AAAA;AAAA;AAAA;AAAA;AAMA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAIA;AACA;AACA;;;;A","sourceRoot":""}